---
title: "Excersice Sheet 7"
fontsize: 11pt
header-includes:
- \usepackage[german]{babel}
- \usepackage{caption}
output:
  pdf_document: default
  html_document: default
highlight: tango
fig_caption: yes
---

\captionsetup[table]{labelformat=empty}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

```{r}
library(stringr)
library(readr)
library(dplyr)

# (Adapt Path)
hprice <- read_csv(str_c(dirname(getwd()), "/Datasets/hprice.csv"))

# The general approach behind each of the examples that we’ll cover below is to:
# 
# 1. Fit a regression model to predict variable (Y).
# 2. Obtain the predicted and residual values associated with each observation on (Y).
# 3. Plot the actual and predicted values of (Y) so that they are distinguishable, but connected.
# 4. Use the residuals to make an aesthetic adjustment (e.g. red colour when residual in very high) to highlight points which are poorly predicted by the model.
#source: https://drsimonj.svbtle.com/visualising-residuals

```


A broker wants to use linear regression to find out which factors have a large influence on the price of a property. For this purpose, the variables described in Table 1 are given for the last 88 sales in the broker's region.

Table: Table 1 House price record

Variabel   | Description
---------  | -------------
`price`    | house price ($\times$ 1,000 EUR)
`bdrms`    | number bedrooms
`lotsize`  | parking area (m$^2$)
`sqrm`     | house area (m$^2$)
`country`  | $==1$ when in country house style
`lprice`   | `log(price)`
`llotsize` | `log(lotsize)`
`lsqrm`    | `log(sqrm)`

1.  Create a linear regression model with `price` as dependent variable and `bdrms`, `lotsize`, `sqrm` und `country`  as independent variables. 
    a) Determine the regression coefficients and  $p$-values of the dependent variable and compare their influence within the model on the predicted value for  `price`. 
    b) Determine how much variance of the dependent variable is explained. 
    c) Check the residuals (graphically) for normal distribution and homoscedasticity.
    
```{r}
# Solution for Task 1...

# Initial implementation & explanation - http://r-statistics.co/Linear-Regression.html 
# Theory & Explanation - http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression7.html
#Explanations for all the graphs (very good) - https://ademos.people.uic.edu/Chapter12.html




#Solution 


#Part - a

lmodel <- lm(price ~ bdrms+lotsize+sqrm+country, data = hprice)
summary(lmodel)
plot(lmodel)
#plot(lmodel, which = 1:4)
# pvalue is at the bottom end (for model) and right most col. of coefficients (for individual variables). 
# Ideal p<0.05.
#We can consider a linear model to be statistically significant only when both these p-Values are less that the pre-determined statistical significance level(0.05).
# For individual variables, the most no. of stars, the more significant they are.

#pvalues are important because that 

#Part - b

#Plot: Residual vs Fitted
#for linearity - the values should not be too far from 0(on +-y axis).Standardized values less than -2 or greater than 2 are deemed problematic)

#R² of 0.02; this means that the model explains only 2% of the data variability.  R² of 0.99, and the model can explain 99% of the total variability.
#Adjusted R² -> 0.66. Therefore 66% of the variability can ve explained by model
#Adjusted R² taken because normal R² increases irrespectively as you increase the no. of independent variables

var(fitted(lmodel)) #variance

#Part - c

#Normal Distribution - QQ plot: Fitting properly. So normality holds here.
#Normality - For any fixed value of X, Y is normally distributed. 
# distribution of studentized residuals
library(MASS)
sresid <- studres(lmodel) 
hist(sresid, freq=FALSE, 
   main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid),max(sresid),length=40) 
yfit<-dnorm(xfit) 
lines(xfit, yfit)


#Homoscedasticity - Residuals should not have pattern (in Scale-Location or in plot(lmodel$residuals))
#If the red line you see on your plot is flat and horizontal with equally and randomly spread data points (like the night sky), you’re good. If your red line has a positive slope to it, or if your data points are not randomly spread out, you’ve violated this assumption.

#Very good doc on Residuals - http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/ 

```

2. Given be the linear regression model from task 1.
    a) Create a scatterplot to display the relationship between the predicted value for `price` and the residual size.
    b) For some houses, the price forecast of the broker model is more than EUR 100,000 off. Highlight houses with a residual size of more than 100 or less than 100. What could be the reasons for high model inaccuracies?
    c) Can the $R^2$-value be increased by using a linear transformation of one of the independent variables?

```{r}
# Solution for Task 2...

plot(hprice$price, pch = 20, col = "blue")
abline(lmodel)

plot(lmodel$residuals, pch = 20, col = "blue")
plot(y= hprice$price, x = lmodel$residuals,  pch = 20, col = "blue")
abline(lmodel)


library(tidyr)
library(ggplot2)
test <- hprice[,1:5]

fit_extra <- lm(price ~ lotsize+sqrm,data = test)

test$predicted <- predict(fit_extra)
test$residuals <- residuals(fit_extra)


# ggplot(test, aes(x = price, y = lotsize+sqrm)) +
#   geom_segment(aes(xend = price, yend = predicted), alpha = .2) +
#   geom_point(aes(color = residuals)) +
#   scale_color_gradient2(low = "blue", mid = "white", high = "red") +
#   guides(color = FALSE) +
#   geom_point(aes(y = predicted), shape = 1) +
#   theme_bw()

test %>% 
  gather(key = "iv", value = "x", -price, -predicted, -residuals) %>%  # Get data into shape
  ggplot(aes(x = x, y = price)) +  # Note use of `x` here and next line
  geom_segment(aes(xend = x, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  facet_grid(~ iv, scales = "free_x") +  # Split panels here by `iv`
  theme_bw()


#part-b
plot(lmodel, which = 1, col=ifelse((lmodel$residuals>100 | lmodel$residuals< -100), 'red', 'blue'))

#part-c
#https://www.datacamp.com/community/tutorials/linear-regression-R 

lmodel1 <- lm(price ~ bdrms+I(log(bdrms))+lotsize+sqrm+country, data = hprice)
#lmodel1 <- lm(price ~ bdrms+I(1/exp(bdrms))+lotsize+sqrm+country, data = hprice)
summary(lmodel)
summary(lmodel1)

```

3. Graphically display the relationship between `bdrms` and `price`. Check whether this relationship is also reflected in the regression model from Task 1. Create a regression model with `bdrms` as the only independent variable. Compare the regression coefficients with those of the model from Task 1 and interpret the differences. 


```{r}
# Solution for Task 3...

plot(x=hprice$bdrms, y=hprice$price)
library(ggstance)

lmodel2 = lm(price~bdrms, data=hprice)
lines(hprice$bdrms, predict(lmodel2), col = 'blue')
summary(lmodel)
summary(lmodel2)

```

------
Dataset:

- http://isgwww.cs.uni-magdeburg.de/cv/lehre/VisAnalytics/material/exercise/datasets/hprice.csv